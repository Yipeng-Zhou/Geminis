# Overview
## Finished Assignments
The Group 15 “Geminis” focuses on the topic “Autonomous Drones”. After 4 weeks’ work, our group has finished the work according to the given project requirements. After starting the simulation, the two drones can automatically and sequentially finish some tasks: take off from the ground, explore the environment, come back to origin after rebuilding is over and land on the ground. The average explore time is normally around 5 min. But it may varies depending on some relevant parameters that we set. The core functions broadly consist of the following parts:
- Perception maps build: Use the information from the unity images to rebuild the 2D-projected maps.
- State machine: Manage the instructions of the drones and show the status of drones.
- Explore: Generate and update frontier, choose current frontier goal based on cost function.
- Path planer: Use the maps to finish the global planning.
- Trajectory planner: Given global path, design a local trajectory.

## Program Flow
This part presents a rough description of the main program logic, aiming at providing a general information about how the main task is done by our program. The detailed description of nodes and ROS packages will be explained in the corresponding part of the report.
<br />
![Program Flow](https://gitlab.lrz.de/yipeng.zhou/intro2ros-project/-/raw/main/images/workflow.png?inline=false)
<br />
The first node unity_ros will transfer the information from the unity environment, where the information is divided into 2 parts, i.e., camera information and drones’ states. The camera information, including the raw pictures and the depth images will be used for generating the point cloud. This process is done by the pkg depth_image_proc. The drones’ states, including the pose and twists of two drones, are used for estimating the current states of drones. The point cloud will be transformed into maps data with the help of the pkg octomap. Then the pkg explore_lite will explore frontiers using the built maps for global planning. After finding the new frontiers in the maps, it decides for two drones where the current goal positions are and send the global path to the node move_base. This node also needs information from point cloud and octomap. It uses the point cloud to detect obstacles and the maps are for global and local planner. move_base publishes the expecting trajectory of the drones. With the desired state sent from the node trajectory\_publisher, the node controller drives the drones and send the simulated state information back to the unity environment. State machine monitors the current states of state machine, mainly from current states of two drones, but also from the stop flags generated by explore. Meanwhile, it sends instructions to trajectory_publisher and controller to execute different movement in different states.
The following parts will present a more detailed description of every node and package. Section 2 shows the maps building process. Section 3 describes frontiers exploration and the global as well as the local planning of the two drones. Section 4 explains how the state machine works.
<br/>

# Two drones fly
For the original source code, we have done following modificiations, described based on Quadrotor_1, but also suitable for Quadrotor_2:
1. unity_ros.cpp, change port and ip, change the time stamp from local time to universal time to match move base time stamp.
2. w_to_unity.cpp, change port and ip .
3. true_state_parser.h, delete useless tf broadcast.
4. state_estimate_corrupt_node.cpp, additionally publish a tf of base coordinate of drone, projecting drone coordinate to the ground, used for move base planning.
5. traj_publisher.cpp, receving command flag form state machine and accordingly publish goal point from explore_lite or user defined goal point.
6. controller_node.cpp, add command flag receving from state machine for activating or deactivating controlling.

For Quadrotor_2, we add the following corresponding source code with suffixes "_2":
```
unity_ros_2.cpp
state_estimate_corrupt_node_2.cpp
controller_node_2.cpp
traj_publisher_2.cpp
w_to_unity_2.cpp
```
<br/>

# SLAM (Simultaneous Localization and Mapping)
Since "Localization" has already been implemented by the source codes from Prof. Ryll ("AutonomousDrones/src/simulation/src/state_estimate_corruptor_node.cpp" broadcasts the coordinate frame "Quadrotor_1" and "AutonomousDrones/src/simulation/src/state_estimate_corruptor_node_2.cpp" broadcasts the coordinate frame "Quadrotor_2", they locate the positions of two quadrotors, respectively), this section will focus on three parts: "Generating point cloud from depth image", "Generating occupancy map from point cloud" and "Merging occupancy maps/projected maps". The provided RViz config "proj.rviz" will make you visualize the results easily.

## Generating point cloud from depth image
To realize this function, we should install the package "depth_image_proc" firstly.
```
sudo apt-get install ros-noetic-depth-image-proc
```
This package is made of "nodelet", subscribes two topics "camera_info (sensor_msgs/CameraInfo)" and "image_rect (sensor_msgs/Image)" and publishes one topic "points (sensor_msgs/PointCloud2)". We need launch two such nodes in "AutonomousDrones/src/simulation/launch/simulation.launch" to convert the depth maps of the two drones into point clouds respectively.

In the launch file, you maybe have found that, these two nodes published the message "PointCloud2" into a same topic "/points", this is the strategy we ended up taking to merge occupancy maps/projected maps. We will explain it in the subsection "Merging occupancy maps/projected maps" later. Besides, these two nodes also published their respective "PointCloud2" messages to another two different topics: "/Quadrotor_1/points" and "/Quadrotor_2/points". These two topics will used in "Navigation" for obstacle detection. To make this conversion successfully, "depth_image_proc" also requires the tf transform between the depth camera and the quadrotor.
```
<!-- frame transfer between Depth camera to Quadrotor_1 -->
<node pkg="tf2_ros" type="static_transform_publisher"
      name="depth_camera_frame_to_quadrotor"
      args="0 0 0 -1.57079632 0 -1.57079632 /Quadrotor_1 /Quadrotor/Sensors/DepthCamera" />

<!-- frame transfer between Depth camera to Quadrotor_2 -->
<node pkg="tf2_ros" type="static_transform_publisher"
      name="depth_camera_frame_to_quadrotor_2"     
      args="0 0 0 -1.57079632 0 -1.57079632 /Quadrotor_2 /Quadrotor/Sensors/DepthCamera" />
```

## Generating occupancy map from point cloud

To realize this function, we should install some related packages firstly.
```
sudo apt-get install ros-noetic-octomap
sudo apt-get install ros-noetic-octomap-mapping
sudo apt-get install ros-noetic-octomap-msgs
sudo apt-get install ros-noetic-octomap-ros
sudo apt-get install ros-noetic-octomap-rviz-plugins
sudo apt-get install ros-noetic-octomap-server
```
The package "octomap_server" subscribes one topic "cloud_in (sensor_msgs/PointCloud2)", which is provided by "depth_image_proc", and can publishes 3D occupancy map "octomap_binary/octomap_full (octomap_msgs/Octomap)" or 2D occupancy map "projected_map (nav_msgs/OccupancyGrid)". Among them, "projected_map" is needed by "Navigation". Since we only use the "PointCloud2" message from topic "/points" in this part, we need only launch one "octomap_server" node to converte it in "AutonomousDrones/src/simulation/launch/simulation.launch". Besides, we also set some parameters to optimize the results.

Finally, you can save the generated 3D occupancy map through the following samll command line tool.
```
(save "octomap_binary":)
rosrun octomap_server octomap_saver mapfile.bt
(save "octomap_full":)
rosrun octomap_server octomap_saver -f mapfile.ot
```
To open the saved file "mapfile.bt" or mapfile.ot", you need install "octovis" firstly,
```
sudo apt-get install ros-noetic-octovis
```
and then run the following command line:
```
octovis mapfile.bt
octovis mapfile.ot
```
<br/>

## Merging occupancy maps/projected maps
In this section, to test the results of merging, the two drones are placed on the different initial positions and fly in circles at the same height as the initial positions:
```
Quadrotor_1/init_pose_x: 10          Quadrotor_2/init_pose_x: 10
Quadrotor_1/init_pose_y: -25         Quadrotor_2/init_pose_y: 15
Quadrotor_1/init_pose_z: 3.16        Quadrotor_2/init_pose_z: 3.16
Quadrotor_1/init_pose_yaw: 0         Quadrotor_2/init_pose_yaw: 0
```
We have tried two technical routes to achieve this functionality: 
1. merging occupancy maps/projected maps
2. merging point clouds at first and then converting this merged point cloud to a occupancy map/projected map.

For each technical route, we have tried several solutions and they all have their own advantages and disadvantages. 

### shared point_cloud topic
As you can see, we finally used the most straightforward way: let the two "depth_image_proc" nodes publish respective "PointCloud2" message to a same topic "/points" and then convert it to an occupancy map/projected map. The shortcoming of this method is that, the point clouds overwrite each other, causing them to appear to flicker randomly when viewed in RViz. But its advantages are also significant: compared to other methods, it's able to generate stable, complete and offset-free occupancy map and projected map.
<br />
![using shared point_cloud topic to merge point clouds](https://gitlab.lrz.de/yipeng.zhou/intro2ros-project/-/raw/main/images/shared_point_cloud_topic.png?inline=false)
<br />

### "point_preprocessor" from Autoware
We also tried to use the package "point_preprocessor" from Autoware, which is the world's leading open-source software project for autonomous driving, to merge point clouds of two "depth_image_proc" nodes (in this method, these two nodes don't publish their "PointCloud2" messages to a same topic), and then convert the merged point cloud to a occupancy map/projected map. To make the package "point_preprocessor" be compiled without the whole Autoware project, we only kept the node "points_concat_filter" of "point_preprocessor", removed unnecessary dependences and modified the files "CMakeLists.txt" and "package.xml". You can check these modification under "AutonomousDrones/src/points_preprocessor". To launch this node, we add the following codes under "AutonomousDrones/src/simulation/launch/simulation.launch", and since this method is not the final strategy that we take, we have deleted these codes in our final "simulation.launch".
```
<!-- merge point clouds using points_preprocessor -->
<node pkg="points_preprocessor" type="points_concat_filter"
      name="points_concat_filter" output="screen">
  <param name="output_frame_id" value="world" />
  <param name="input_topics" value="[/Quadrotor_1/points, /Quadrotor_2/points" />
  <remap from="/points_concat" to="/points_concat" />
</node>
```
This method merged two point clouds perfectly and we will not see flickers anymore. However, this package is designed based on assuming two sensors on one robot, so the optimal "output_frame_id" should be "base_link" of the single robot. However, we have two drones with different "base_link", so we can only let the coordinate frame "world" be the "output_frame_id", which will make the final converted projected map be centered on the origin of "world", shift from those projected maps generated by the unmerged point clouds and become incomplete. It is because of these shortcomings that we abandon this method. The image below shows both the projected map generated by the merged point cloud (down and right) and the projected maps generated the unmerged point clouds (above):
<br />
![using "point_preprocessor" to merge point clouds](https://gitlab.lrz.de/yipeng.zhou/intro2ros-project/-/raw/main/images/point_preprocessor.png?inline=false)
<br />

### "multirobot_map_merge" from "m-explore"
We also tried to merge two projected maps generated by different point clouds (in this method, these two nodes don't publish their "PointCloud2" messages to a same topic but two different topics "/Quadrotor_1/points" and "/Quadrotor_2/points", then two "octomap_server" nodes converte each point cloud to a projected map). The package "multirobot_map_merge" from "m-explore" will help us to realize it, so install this package firstly.
```
sudo apt install ros-neotic-multirobot-map-merge
```
"multirobot_map_merge" has two merging modes: "merging with known initial positions" and "merging without known initial positions". We use the first mode, launch the node and set the initial positions of two drones in "AutonomousDrones/src/simulation/launch/simulation.launch". It is worth noting that, the initial positions should be defined in the namespace of each robot and other parameters should be defined in the namespace of this node. And since this method is not the final strategy that we take, we have deleted these codes in our final "simulation.launch".
```
<!-- merge maps using multirobot_map_merge -->
<param name="Quadrotor/map_merge/init_pose_x" value="10"/>
<param name="Quadrotor/map_merge/init_pose_y" value="-25"/>
<param name="Quadrotor/map_merge/init_pose_z" value="3.16"/>
<param name="Quadrotor/map_merge/init_pose_yaw" value="0"/>
<param name="Quadrotor2/map_merge/init_pose_x" value="10"/>
<param name="Quadrotor2/map_merge/init_pose_y" value="15"/>
<param name="Quadrotor2/map_merge/init_pose_z" value="3.16"/>
<param name="Quadrotor2/map_merge/init_pose_yaw" value="0"/>

<group ns="map_merge">
<node pkg="multirobot_map_merge" type="map_merge" respawn="false" name="map_merge" output="screen">
  <param name="robot_map_topic" value="projected_map"/>
  <param name="robot_namespace" value=""/>  
  <param name="merged_map_topic" value="map_merge"/>
  <param name="world_frame" value="world"/>
  <param name="known_init_poses" value="true"/>
  <param name="merging_rate" value="0.5"/>
  <param name="discovery_rate" value="0.05"/>
  <param name="estimation_rate" value="0.1"/>
  <param name="estimation_confidence" value="1.0"/>
</node>
</group>
```

If you check the source code of this package, you can find that the merged projected map is also centered on the origin of the coordinate frame "world", which will cause the same problems as "point_preprocessor" from Autoware: generated projected map shifts from those projected maps generated by the unmerged point clouds and become incomplete. Besides, this package cannot merge 3D occupancy maps. So this method is also not a good choice. Similarly, the image below shows both the merged projected map (down and right) and the unmerged projected maps (above):
<br />
![using "multirobot_map_merge" to merge projected map](https://gitlab.lrz.de/yipeng.zhou/intro2ros-project/-/raw/main/images/multirobot_map_merge.png?inline=false)
<br />

### homemade package "map_stack"
To avoid the problems in "multirobot_map_merge", we also write a package "map_stack" by ourself to merge two different generated projected maps (in this method, these two nodes don't publish their "PointCloud2" messages to a same topic but two different topics "/Quadrotor_1/points" and "/Quadrotor_2/points"). During scanning mission, our two drones will send their processed point cloud message to two separate Ocotomap Server node and then two projected maps will be generated accordingly. This Map_stack node will subscribes to these two map topics which are Occupancy Grid message. After processing it will publish one single topic which is also Occupancy Grid i.e., the merged map. 
Since these two original costmap from our drones are simply squares or rectangles seen in rviz, our ideal is to build a bigger square or rectangle map that contains these two maps. Steps like following:
    1. Based on the features of Occupancy Grid message type, we first find a new origin point by comparing the origin points of two input maps. 
    2. Then calculate new width and height to enclose two input maps. 
    3. Initialize a new Occupancy Grid message given new origin point ,width and height.
    4. Stack two input maps’ information up on the new Occupancy Grid. Choose max value for overlapped space.
    5. Publish new map message
Since merging point cloud can help us built 3D Occupancy grid, we choose the merging clouds method as the final choice.
<br/>

# Navigation
This section contains two parts: explore_lite and move_base.
## explore_lite
For the exploration task we use the package explore_lite as basement. This package was originally developed for one robot exploration.
<br/>
How explore_lite work:

   1. Search frontiers base on the known map. Frontiers are boundaries of the map that are not occupied by obstacles, which means that there maybe     

      unknown spaces near these frontiers.
   2. Sort available frontiers base on a cost function:
      cost = (α * distance between frontier center and robot)  - (β * frontier size)
      Frontiers will be sorted from small cost to large cost. Small cost means the frontier is big and closer to the robot. 
   3. Choose the first frontier as goal frontier. Send the centroid point of this frontier as goal point to move_base.
   4. Once the map update, find new frontiers.
   5. Back to step 2 and iterate until there is no frontier found in the map.
<br/>

Above is the basic function loop of explore_lite. There are also some built in protection to prevent robot from chasing an unreachable goal.

But the original version can’t fulfill our mission. This package has to be modified to be suitable for two drone exploration. Following is how we modify the source code (AutonomousDrones/src/explore):

1. We successfully deploy the explore_lite package on our first quadrotor, making it’s able to accomplish the exploration mission alone.  
2. Then we have to additionally add connection between explore_lite node to move_base for Quadrotor 2 in the construct function of explore class.
3. In step 2, we replace the centroid point of frontier by middle point as the goal point sent to robot. The centroid point is calculated by an average operation on all points laying on the frontier. Sometime it is kind far away from the frontier and due to the constrain of camera range, the robot is unable to detected the region behind this frontier. Middle point otherwise lies on the frontier. After we change the goal point to middle point. The exploration performance is more stable.
4. We simply choose the first frontier of the frontiers list, which is the frontier with the small cost and set it’s middle point as goal point sending to   move_base of quadrotor 1 and choose the last frontier of the frontiers list, which is the frontier with the largest cost and set it’s middle point as goal point sending to move_base of quadrotor 2. Since the cost function is calculated only base on our first quadrotor, second quadrotor’s position is not considered in the cost function. To make the choose more intuitive, we set the distance gain factor bigger. By doing so, the first quadrotor tends to explore the frontiers closed to it and the second quadrotor will search the frontiers far away from first quadrotor. As for now, our two drones are able to together accomplish the explore mission. But the goal choose for second drone is too simple that sometime it will waste time wandering around or pending.
5. However, there is a problem that the second quadrotor always gets stuck at somewhere on the map. In this case the planners of the second quadrotor does not work. To resolve the problem we tried to use two nodes of explore_lite to  realize the exploration of  two quadrotors respectively. As each quadrotor uses move_base in its own namespace, the explore_lite for each quadrotor maintains a move_base client in the corresponding namespace. Compared with the first approach mentioned above, each quadrotor constructs frontiers separately and selects its own goals. The distance gain factor is set to be zero in order to only consider the distance between the frontier and the quadrotor when selecting the goal. In this way there is no need to send different goals manually to the two move_base because for each quadrotor the costs are calculated and the frontier with lowest cost will very possible be different as long as the positions of the quadrotors are different. The advantage is that the goals are more stable and can prevent the wandering problem mentioned above. As the client of move_base are constructed under different namespace, this approach may avoid the conflict when sending the goal and is hopefully able to solve the problem of getting stuck at somewhere on the map. In the practice, the probability of getting stuck is reduced and the flight is more stable. That is why we choose this approach in the final version the code.
<br/>

## move_base
For the navigation task the navigation stack is used. The major node in this stack is the node move_base. This node constructs and maintains the global and local costmaps using the message from topic “/projected_map”. With the costmaps and the odometry data of the quadrotors, “global planner” of move_base publishes global path and “local planner” of move_base publishes local trajectories for each quadrotor, which navigate the quadrotors from the current position to the goal position set by the explore_lite. The structure of the navigation stack is illustrated below:
<br />
![navigation stack](https://gitlab.lrz.de/yipeng.zhou/intro2ros-project/-/raw/main/images/move_base.png?inline=false)
<br />
In detail, the global path published by “global planner” is planned using the global costmap, which only contains the relatively sparse points representing the milestones for the quadrotor to achieve from the current position to the goal position. The local trajectory published by “local planner” is planned based on the global path and the local costmap, trying to follow this global path. The local trajectory contains a set of dense points containing the position and the velocity that should be followed by the controller. Compared with the global path, the local trajectory is much shorter and only covers a short distance from the current position of the quadrotor. With the local trajectory the quadrotor can avoid the dynamical obstacles and adapt the dynamical environment.
For global planner, we choose A\* algorithm, while for local planner we have tested  DWA(Dynamic Window Approach) Planner and TEB(Time Elastic Band) Planner. However, eventually the DWA local planner is chosen as the local planner used in the final version because it is simpler, contains less parameters and more compatible with the state machine that we used.
The code of the node move_base is implemented by ROS and can be regarded as an external package for our project. And we setting the parameters of the costmaps and the planners through .yaml files under “AutonomousDrones/src/simulation/param”. Besides, we configure two move_base under two namespaces, constructing costmaps and planning trajectories for each quadrotors separately, in order to navigate each quadrotor to different goals separately. But there is one thing to be noted that the projected map used to construct the costmaps for the quadrotors are the same in two different move_base, which is as the information of the environment.
After the trajectories are planned, the controller will control the quadrotors to follow the local trajectory. The code of the controller is already provided and can be regarded as the external code for the project. To publish the desired state to the controller, we implement our own code “traj_punlisher.cpp" or "traj_punlisher_2.cpp” to subscribe the topic from the local planner and publish command to controller . As mentioned above, the local planner publishes a set of dense points containing the desired positions and velocities for the controller to follower. We take only one point at a time as the desired state and publish it. In our application this point is set to be at the 80% length of the whole local trajectory.
<br/>

# State Machine
1. Goal from acquirements: A state machine for your robot, managing the takeoff, travelling and landing at the goal location.

2. Tool: Smach (based on Python)

3. Possible Packages need to be installed: 

   ```
   For Smach State Machine: 
   sudo apt-get install python3
   sudo apt-get install ros-noetic-smach
   For Smach_Viewer:
   sudo apt-get python3-wxgtk4.0
   sudo apt-get install xdot
   ```

4. Added nodes and topics relevant to this part:
    <br />
    ![Nodes and topics of State Machine](https://gitlab.lrz.de/yipeng.zhou/intro2ros-project/-/raw/main/images/state_machine.png?inline=false)
    <br />

5. Node description:

   a). *smach_state_machine*: A top state machine container, which contains a Start state and a concurrent state machine for two drones. It subscribes mainly the current locations of two drones as well as the stop flag after finishing the scanning process. According to these signals, it will enter the states sequentially from the top and publishes different state triggers for each drone. It will stop after both two drones finish their task.

   b). *smach_viewer*: A graphical interface which will monitor and display the status of state machine

   c). *traj_publisher* and *traj_publisher_2*: Depending on different state triggers, they will publish the desired goals for taking off, current goals generated from move_base during the scanning and back navigation as well as the landing goals.

   d). *back_navigation_1* and *back_navigation_2*: Nodes using the MoveBaseClient and actionlib to generate a global and local path to the goal

   e). *controller_node* and *controller_node_2*: A state trigger for starting is added to them.

   f). *explore*: After finishing the scanning, it will publish a stop flag to the state machine.

6. Problems:

   a). Since the local planner and global planner from move_base are initialized at the beginning, but during the taking off process the drones will not executing the command from them. It will cause that the previous local planner TebLocalPlanner may stop during the taking off process. So, we choose another local planner for that.
   <br/>
   b). If the controller nodes publish the commands in the beginning without receiving the desired state from the trajectory publishers, the drones will fly against wall. Therefore, we add a start trigger generated from state machine to controller nodes.
   <br/>
   c). During the back navigation process, the succeeded flag after reaching the goal will sometimes not published. So, the state machine will directly get the current states of two drones to judge if they achieved the goal location.
   <br/>
    d). Due to the conflicts between the smach_viewer package and the version of Python, we cannot simply install the ros-noetic-smach_viewer. So, we added the source code of smach_viewer, which is compatible with ROS Noetic and Python3.
   
7. State Machine Workflow:
<br />
![State Machine Workflow](https://gitlab.lrz.de/yipeng.zhou/intro2ros-project/-/raw/main/images/state%20machine%20workflow.png?inline=false)
<br />
